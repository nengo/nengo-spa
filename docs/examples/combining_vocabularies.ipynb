{
 "metadata": {
  "kernelspec": {
   "display_name": "nengodev",
   "language": "python",
   "name": "nengodev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Combining vocabularies\n",
      "\n",
      "Sometimes a model will have distinct vocabularies, for example one for shapes and one for colors, but in certain places you might need to combine these vocabularies into larger vocabularies. This examples demonstrates different use cases and possibilities to do so."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "import nengo\n",
      "import nengo_spa as spa"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Subsets\n",
      "\n",
      "It is possible to create a subset from a vocabulary containing only specific keys. This can be helpful for excluding some Semantic Pointers from a cleanup (associative) memory."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vocab = spa.Vocabulary(64)\n",
      "vocab.populate('A; B; C; X; Y; Z')\n",
      "\n",
      "with spa.Network() as model:\n",
      "    stimulus = spa.Transcode('(A + X).normalized()', output_vocab=vocab)\n",
      "    cleanup = spa.ThresholdingAssocMem(0.3, vocab.create_subset(['A', 'B', 'C']))\n",
      "    stimulus >> cleanup\n",
      "    p = nengo.Probe(cleanup.output, synapse=0.03)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with nengo.Simulator(model) as sim:\n",
      "    sim.run(0.5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(sim.trange(), spa.similarity(sim.data[p], vocab))\n",
      "plt.xlabel(\"Time [s]\")\n",
      "plt.ylabel(\"Similarity\")\n",
      "plt.legend(vocab.keys())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Union of vocabularies\n",
      "\n",
      "In the previous example we started with the complete vocabulary and derived the subset from it. Sometimes it is easier to start out with the smaller vocabularies and derive the larger one from it. To create the union of two vocabularies use the function `combine_vocabs`. By default, it will check that the constraint on the maximum similarity is not exceeded in the combined vocabulary."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "abc_vocab = spa.Vocabulary(64)\n",
      "abc_vocab.populate('A; B; C')\n",
      "\n",
      "xyz_vocab = spa.Vocabulary(64)\n",
      "xyz_vocab.populate('X; Y; Z')\n",
      "\n",
      "combined_vocab = spa.combine_vocabs((abc_vocab, xyz_vocab))\n",
      "\n",
      "with spa.Network() as model:\n",
      "    stimulus = spa.Transcode('(A + X).normalized()', output_vocab=combined_vocab)\n",
      "    cleanup = spa.ThresholdingAssocMem(0.3, abc_vocab)\n",
      "    stimulus >> cleanup\n",
      "    p = nengo.Probe(cleanup.output, synapse=0.03)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with nengo.Simulator(model) as sim:\n",
      "    sim.run(0.5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(sim.trange(), spa.similarity(sim.data[p], combined_vocab))\n",
      "plt.xlabel(\"Time [s]\")\n",
      "plt.ylabel(\"Similarity\")\n",
      "plt.legend(combined_vocab.keys())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Combinations of bound Semantic Pointers\n",
      "\n",
      "Assume we construct simple scene representations like `s = RED * CIRCLE + BLUE * SQUARE` and manipulate them (for example `s * (~RED * BLUE + ~BLUE * RED)` to swap colors). This introduces quite some noise that we might want to cleanup, but the cleanup needs to be to bound pairs of Semantic Pointers. Let us start by defining vocabularies for colors and shapes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v_colors = spa.Vocabulary(64)\n",
      "v_colors.populate('RED; BLUE')\n",
      "\n",
      "v_shapes = spa.Vocabulary(64)\n",
      "v_shapes.populate('CIRCLE; SQUARE')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 93
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then we can create a vocabulary of bound pairs. Note that by default `_` is used to combine the keys."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v_colored_objs = spa.pair_vocabs((v_colors, v_shapes))\n",
      "print(list(v_colored_objs.keys()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 94
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with spa.Network() as model:\n",
      "    scene = spa.State(v_colored_objs)\n",
      "    spa.sym('RED_CIRCLE + BLUE_SQUARE') >> scene\n",
      "    \n",
      "    manipulation = spa.State(spa.combine_vocabs((v_colors, v_shapes)))\n",
      "    spa.sym('~RED * BLUE + ~BLUE * RED') >> manipulation\n",
      "    \n",
      "    cleanup = spa.ThresholdingAssocMem(0.2, v_colored_objs, function=lambda x: x > 0) \n",
      "    scene * manipulation.reinterpret(v_colored_objs) >> cleanup\n",
      "    \n",
      "    p_in = nengo.Probe(cleanup.input, synapse=0.03)\n",
      "    p_out = nengo.Probe(cleanup.output, synapse=0.03)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 95
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with nengo.Simulator(model) as sim:\n",
      "    sim.run(0.5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 96
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(12, 4))\n",
      "\n",
      "plt.subplot(1, 2, 1)\n",
      "plt.plot(sim.trange(), spa.similarity(sim.data[p_in], v_colored_objs))\n",
      "plt.title(\"Before cleanup\")\n",
      "plt.xlabel(\"Time [s]\")\n",
      "plt.ylabel(\"Similarity\")\n",
      "plt.legend(v_colored_objs.keys(), loc='upper left')\n",
      "\n",
      "plt.subplot(1, 2, 2)\n",
      "plt.plot(sim.trange(), spa.similarity(sim.data[p_out], v_colored_objs))\n",
      "plt.title(\"After cleanup\")\n",
      "plt.xlabel(\"Time [s]\")\n",
      "plt.ylabel(\"Similarity\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 97
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Tracking of vocabulary relationships\n",
      "\n",
      "When using `create_subset`, `combine_vocabs`, and `pair_vocabs`, the vocabularies track how they are related. That means modules using sub-/supersets of vocabularies can be connected without explicit `reinterpret`. Same when doing a circular convolution of two modules with different vocabularies. For example, the following is valid:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v1 = spa.Vocabulary(64)\n",
      "v1.populate('A; B')\n",
      "\n",
      "v2 = spa.Vocabulary(64)\n",
      "v2.populate('C; D')\n",
      "\n",
      "combined = spa.combine_vocabs((v1, v2))\n",
      "paired = spa.pair_vocabs((v1, v2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with spa.Network() as model:\n",
      "    state1 = spa.State(v1)\n",
      "    state2 = spa.State(v2)\n",
      "    \n",
      "    state_combined = spa.State(combined)\n",
      "    state1 >> state_combined  # the vocabularies of the state differ, but are in a subset/superset relation\n",
      "    state_combined >> state2\n",
      "    \n",
      "    bound = spa.State(paired)\n",
      "    state1 * state2 >> bound  # state1 and state2 have different vocabularies,\n",
      "                              # but produce the vocabulary of bind with circular convolution"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    }
   ],
   "metadata": {}
  }
 ]
}

